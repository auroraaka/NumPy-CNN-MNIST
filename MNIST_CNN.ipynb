{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "142MobFdSJlRUI7fRGJhDq6VTQtOGniPa",
      "authorship_tag": "ABX9TyOVzyH6um+stslpKCoUwS8t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auroraaka/NumPy-CNN-MNIST/blob/main/MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "## HELPER FUNCTIONS ##\n",
        "\n",
        "def save_model_parameters(epoch, path, **parameters):\n",
        "    epoch_path = os.path.join(path, f'epoch_{epoch+1}')\n",
        "    os.makedirs(epoch_path, exist_ok=True)\n",
        "    for name, values in parameters.items():\n",
        "        np.save(os.path.join(epoch_path, f'{name}.npy'), values)\n",
        "\n",
        "def load_model_parameters(path, epoch=None, last_epoch=False, **parameters):\n",
        "    if last_epoch:\n",
        "        subdirs = [os.path.join(path, d) for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
        "        epoch_dirs = [d for d in subdirs if re.match(r'.*/epoch_\\d+', d)]\n",
        "        epoch_numbers = [int(re.search(r'epoch_(\\d+)', d).group(1)) for d in epoch_dirs]\n",
        "        epoch = max(epoch_numbers)\n",
        "\n",
        "    epoch_path = os.path.join(path, f'epoch_{epoch}')\n",
        "    for name in parameters.keys():\n",
        "        parameters[name] = np.load(os.path.join(epoch_path, f'{name}.npy'))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def convolve(X, filters):\n",
        "    num_filters, filter_h, filter_w = filters.shape\n",
        "    X_h, X_w = X.shape\n",
        "\n",
        "    feature_map_h, feature_map_w = X_h - filter_h + 1,  X_w - filter_w + 1\n",
        "    feature_map = np.zeros((num_filters, feature_map_h, feature_map_w))\n",
        "\n",
        "    for filter_index in range(num_filters):\n",
        "        for x in range(feature_map_w):\n",
        "            for y in range(feature_map_h):\n",
        "                feature_map[filter_index, y, x] = np.sum(X[y: y + filter_h, x: x + filter_w] * filters[filter_index])\n",
        "\n",
        "    return feature_map\n",
        "\n",
        "def pool(X, pool=(2, 2), stride=2, type='max'):\n",
        "    num_maps, X_h, X_w = X.shape\n",
        "    pool_h, pool_w = pool\n",
        "\n",
        "    downsampled_h, downsampled_w = int((X_h - pool_h) / stride) + 1, int((X_w - pool_w) / stride) + 1\n",
        "    downsampled = np.zeros((num_maps, downsampled_h, downsampled_w))\n",
        "    max_indices = np.zeros((num_maps, downsampled_h, downsampled_w, 2), dtype=int)\n",
        "\n",
        "    for m in range(num_maps):\n",
        "        for x in range(0, X_w, stride):\n",
        "            for y in range(0, X_h, stride):\n",
        "                if type == 'max':\n",
        "                    downsampled[m, int(y/stride), int(x/stride)] = np.max(X[m, y: y + pool_h, x: x + pool_w])\n",
        "                    max_y, max_x = np.unravel_index(np.argmax(X[m, y: y + pool_h, x: x + pool_w]), (pool_h, pool_w))\n",
        "                    max_indices[m, int(y/stride), int(x/stride)] = [y + max_y, x + max_x]\n",
        "    return downsampled, max_indices\n",
        "\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def grad_ReLU(z):\n",
        "    return np.where(z > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / np.sum(e_x, axis=0)\n",
        "\n",
        "def dropout_mask(shape, dropout_rate=0.1):\n",
        "    unscaled_mask = np.random.rand(*shape) > dropout_rate\n",
        "    scaled_mask = unscaled_mask / (1.0 - dropout_rate)\n",
        "    return unscaled_mask, scaled_mask\n",
        "\n",
        "def he_initialization_conv(filter_dim):\n",
        "    filter_size, _, num_filters = filter_dim\n",
        "    num_inputs_per_filter = filter_size ** 2\n",
        "    return np.random.randn(num_filters, filter_size, filter_size) * np.sqrt(2.0 / num_inputs_per_filter)\n",
        "\n",
        "def he_initialization_fc(input_dim, filter_dim, num_outputs):\n",
        "    input_size, _, _ = input_dim\n",
        "    filter_size, _, num_filters = filter_dim\n",
        "    num_inputs_per_filter = int((input_size - filter_size + 1) / 2) ** 2\n",
        "    return np.random.randn(num_outputs, num_inputs_per_filter * num_filters) * np.sqrt(2.0 / num_inputs_per_filter)\n",
        "\n",
        "def he_initialization_out(num_neurons, num_outputs):\n",
        "    return np.random.randn(num_outputs, num_neurons) * np.sqrt(2.0 / num_neurons)\n",
        "\n",
        "def initialize_parameters(input_dim, filter_dim, num_neurons, num_outputs):\n",
        "    conv_weights = he_initialization_conv(filter_dim)\n",
        "    conv_biases = np.zeros((filter_dim[2], 1))\n",
        "    fc_weights = he_initialization_fc(input_dim, filter_dim, num_neurons)\n",
        "    fc_biases = np.zeros(num_neurons)\n",
        "    output_weights = he_initialization_out(num_neurons, num_outputs)\n",
        "    output_biases = np.zeros(num_outputs)\n",
        "\n",
        "    return conv_weights, conv_biases, fc_weights, fc_biases, output_weights, output_biases\n",
        "\n",
        "def one_hot_encode(label, output_size=10):\n",
        "    one_hot = np.zeros(output_size)\n",
        "    one_hot[label] = 1\n",
        "    return one_hot\n",
        "\n",
        "def learning_rate_optimizer(initial_learning_rate, epoch, decay_rate=0.1):\n",
        "    return initial_learning_rate * np.exp(-decay_rate * epoch)\n",
        "\n",
        "def forward_propogation(image, conv_weights, conv_biases, fc_weights, fc_biases, output_weights, output_biases, dropout=True):\n",
        "    z = convolve(image, conv_weights + conv_biases.reshape(-1, 1, 1))\n",
        "    conv_out = ReLU(z)\n",
        "    pool_out, max_indices = pool(conv_out)\n",
        "    flattened = pool_out.flatten()\n",
        "    y = np.dot(fc_weights, flattened) + fc_biases\n",
        "    fc_out = ReLU(y)\n",
        "    unscaled_mask, scaled_mask = dropout_mask(fc_out.shape)\n",
        "    if dropout:\n",
        "        fc_out *= scaled_mask\n",
        "    output_out = softmax(np.dot(output_weights, fc_out) + output_biases)\n",
        "    predicted_value = np.argmax(output_out)\n",
        "    return z, conv_out, pool_out, max_indices, flattened, unscaled_mask, y, fc_out, output_out, predicted_value\n",
        "\n",
        "def cross_entropy_loss(output_out, label):\n",
        "    epsilon = 1e-8\n",
        "    ce_loss = -np.sum(one_hot_encode(label) * np.log(output_out + epsilon))\n",
        "    return np.mean(ce_loss)\n",
        "\n",
        "def cross_entropy_loss_differential(output_out, label):\n",
        "    return output_out - one_hot_encode(label)\n",
        "\n",
        "def backprop_pool(conv_out, conv_dL_pooled, pooled_dim, pool_dim, max_indices, pool_type='max'):\n",
        "    pool_h, pool_w, stride = pool_dim\n",
        "    pooled_h, pooled_w, num_maps = pooled_dim\n",
        "    conv_dL = np.zeros_like(conv_out)\n",
        "    for m in range(num_maps):\n",
        "        for x in range(0, pooled_w, stride):\n",
        "            for y in range(0, pooled_h, stride):\n",
        "                if pool_type == 'average':\n",
        "                    conv_dL[m, y: y + pool_h, x: x + pool_w] = conv_dL_pooled[m, int(y/stride), int(x/stride)] / (pool_h * pool_w)\n",
        "                if pool_type == 'max':\n",
        "                    h, w = max_indices[m, y, x]\n",
        "                    conv_dL[m, h, w] = conv_dL_pooled[m, int(y/stride), int(x/stride)]\n",
        "    return conv_dL\n",
        "\n",
        "def backpropogation(image, label, output_weights, fc_weights, pooled_dim, pool_dim, z, conv_out, pool_out, max_indices, flattened, unscaled_mask, y, fc_out, output_out, dropout=True):\n",
        "    ce_loss = cross_entropy_loss(output_out, label)\n",
        "    output_dL = cross_entropy_loss_differential(output_out, label)\n",
        "    output_db = output_dL\n",
        "    output_dW = np.outer(output_dL, fc_out)\n",
        "\n",
        "    fc_dL = np.dot(output_weights.T, output_dL) * grad_ReLU(y)\n",
        "    if dropout:\n",
        "        fc_dL *= unscaled_mask\n",
        "    fc_db = fc_dL\n",
        "    fc_dW = np.outer(fc_dL, flattened)\n",
        "\n",
        "    conv_dL_pooled = np.dot(fc_dL, fc_weights).reshape(pool_out.shape)\n",
        "    conv_dL = backprop_pool(conv_out, conv_dL_pooled, pooled_dim, pool_dim, max_indices, pool_type='max')\n",
        "    conv_dLdz = grad_ReLU(z) * conv_dL\n",
        "    conv_db = np.sum(conv_dLdz, axis=(1,2)).reshape(-1,1)\n",
        "    conv_dW = convolve(np.flip(np.flip(image, 0), 1), conv_dLdz)\n",
        "\n",
        "    return ce_loss, output_dW, output_db, fc_db, fc_dW, conv_dW, conv_db\n"
      ],
      "metadata": {
        "id": "onl9x4ER92bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
        "\n",
        "## INITIALIZATION ##\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = (np.expand_dims(train_images, axis=-1)/255.).astype(np.float32)\n",
        "train_labels = (train_labels).astype(np.int64)\n",
        "test_images = (np.expand_dims(test_images, axis=-1)/255.).astype(np.float32)\n",
        "test_labels = (test_labels).astype(np.int64)\n",
        "\n",
        "path = '/content/drive/MyDrive/model_parameters'\n",
        "\n",
        "num_inputs = len(train_images)\n",
        "input_dim = (28, 28, 1)\n",
        "input_size, _, input_depth = input_dim\n",
        "filter_dim = (3, 3, 32)\n",
        "filter_size, _, num_filters = filter_dim\n",
        "pool_dim = (2, 2, 2)\n",
        "pool_size, _, stride = pool_dim\n",
        "pooled_dim = (13, 13, 32)\n",
        "pooled_size, _, num_maps = pooled_dim\n",
        "num_neurons = 100\n",
        "num_outputs = 10\n",
        "\n",
        "conv_weights, conv_biases, fc_weights, fc_biases, output_weights, output_biases = initialize_parameters(input_dim, filter_dim, num_neurons, num_outputs)\n",
        "initial_learning_rate = 1e-3\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "V6_i9j0A-iVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TRAINING ##\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    loss = []\n",
        "    total_correct = 0\n",
        "    learning_rate = learning_rate_optimizer(initial_learning_rate, epoch, decay_rate=0.1)\n",
        "\n",
        "    for i in range(num_inputs):\n",
        "        n = random.randint(0, num_inputs-1)\n",
        "        image = train_images[n][:].reshape(28, 28)\n",
        "        label = train_labels[n]\n",
        "\n",
        "        # Forward Propogation #\n",
        "        z, conv_out, pool_out, max_indices, flattened, unscaled_mask, y, fc_out, output_out, predicted_value = forward_propogation(image, conv_weights, conv_biases, fc_weights, fc_biases, output_weights, output_biases)\n",
        "\n",
        "        # Backpropogation #\n",
        "        ce_loss, output_dW, output_db, fc_db, fc_dW, conv_dW, conv_db = backpropogation(image, label, output_weights, fc_weights, pooled_dim, pool_dim, z, conv_out, pool_out, max_indices, flattened, unscaled_mask, y, fc_out, output_out)\n",
        "\n",
        "        # Parameter Update #\n",
        "        output_biases = output_biases - learning_rate * output_db\n",
        "        output_weights = output_weights - learning_rate * output_dW\n",
        "        fc_biases = fc_biases - learning_rate * fc_db\n",
        "        fc_weights = fc_weights - learning_rate * fc_dW\n",
        "        conv_biases = conv_biases - learning_rate * conv_db\n",
        "        conv_weights = conv_weights - learning_rate * conv_dW\n",
        "\n",
        "        # Plotting #\n",
        "        if (predicted_value == label):\n",
        "            total_correct += 1\n",
        "\n",
        "        loss.append(ce_loss)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"(Epoch: {epoch+1}, Iteration: {i}) => Training Accuracy: {total_correct/len(train_images)*100:.2f}%, Average Loss: {round(np.average(loss[-100]), 5) if len(loss) > 100 else 0}\")\n",
        "\n",
        "\n",
        "    save_model_parameters(epoch, path, conv_weights=conv_weights, conv_biases=conv_biases, fc_weights=fc_weights, fc_biases=fc_biases, output_weights=output_weights, output_biases=output_biases)\n"
      ],
      "metadata": {
        "id": "4S5djj_q_YA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TESTING ##\n",
        "\n",
        "params = load_model_parameters(path, last_epoch=True, conv_weights=None, conv_biases=None, fc_weights=None, fc_biases=None, output_weights=None, output_biases=None)\n",
        "conv_weights = params['conv_weights']\n",
        "conv_biases = params['conv_biases']\n",
        "fc_weights = params['fc_weights']\n",
        "fc_biases = params['fc_biases']\n",
        "output_weights = params['output_weights']\n",
        "output_biases = params['output_biases']\n",
        "\n",
        "num_tests = len(test_images)\n",
        "num_correct_predictions = 0\n",
        "\n",
        "for i in range(num_tests):\n",
        "    image = test_images[i][:].reshape(28, 28)\n",
        "    label = test_labels[i]\n",
        "\n",
        "    *_, predicted_value = forward_propogation(image, conv_weights, conv_biases, fc_weights, fc_biases, output_weights, output_biases, dropout=False)\n",
        "    if predicted_value == label:\n",
        "        num_correct_predictions += 1\n",
        "\n",
        "accuracy = num_correct_predictions / num_tests\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "xtTuJygM_qgd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}